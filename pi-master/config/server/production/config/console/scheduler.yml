regularly_analyze_free_text_answers:
  every: "10m"
  class: RegularlyAnalyzeFreeTextAnswers
  queue: default
  description: "This job will regularly analyze free text answers"

scheduled_tag_automation_worker:
  # every 15 minutes
  cron: '*/15 * * * *'
  class: ScheduledTagAutomationWorker
  queue: default
  description: "This job labels answers with tags through the OPEN AI API"

account_stats:
  # every hour
  cron: '0 * * * *'
  class: AccountStatsWorker
  queue: default
  description: "This job fills account_stats table with data from InfluxDB every hour"

bundle_audit_worker:
  cron: '0 15 * * *'
  class: BundleAuditWorker
  queue: default
  description: "This job checks for security vulnerabilities in our gems every day"

yarn_audit_worker:
  cron: '0 20 * * *'
  class: YarnAuditWorker
  queue: default
  description: "This job checks for security vulnerabilities in our yarn packages every day"

trim_sessions:
  every: "30d"
  class: TrimSessionsWorker
  queue: default
  description: "This job will trim session every 30 days"

# Unpause when Comcast has sorted out their error 403 issues
# melee:
#   cron: '0 * * * *'
#   class: MeleeWorker
#   queue: default
#   description: "This job exports Melee data every hour"

astra_zeneca:
  cron: '0 5 * * *'
  class: AstraZenecaWorker
  queue: console
  description: "This job sends a CSV file to S3 on a daily basis"

astra_zeneca_aggregate:
  cron: '0 13 * * *'
  class: AstraZenecaAggregateWorker
  queue: console
  description: "This job sends a CSV file of aggregated survey data to AstraZeneca's S3 on a daily basis"

free_text_translator:
  every: '5m'
  class: FreeTextTranslator
  queue: console
  description: "This job translate free text answers created within 5 minutes"

scheduled_report_checkup:
  cron: '0 23 * * *'
  class: ScheduledReportCheckupWorker
  queue: default
  description: "This job checks the status of scheduled reports and notifies us when one has failed"

ben_moore:
  cron: '0 9 1 * *'
  class: PeriodicReportWorkers::BenMooreWorker
  queue: console
  description: "This job sends submission-level data to PI's S3 on a monthly basis"

worker_output_summary:
  cron: '15 0 * * *'
  class: WorkerOutputSummaryWorker
  queue: default
  description: "This job sends the urls of worker output on S3 on a daily basis"

deactivate_users:
  cron: "0 0 * * *"
  class: DeactivateExpiredUsersWorker
  queue: default
  description: "This job will daily check expired users and deactivate them"

complete_expired_surveys:
  cron: '0 * * * *'
  class: CompleteExpiredSurveysWorker
  queue: default
  description: "This job checks for live surveys with expired end dates and marks them as complete every hour"

new_jersey_transit:
  cron: '0 15 * * 1'
  class: NewJerseyTransitWorker
  queue: default
  description: "This job sends data to NJT's Hubspot every Monday"

# https://gitlab.ekohe.com/ekohe/pulseinsights/pi/-/issues/2618#note_1184958
# nba_email_survey_worker:
#   cron: '30 1 * * *'
#   class: NBAEmailSurveyWorker
#   queue: default
#   description: "This job sends submission data to the NBA daily"

azurity_worker:
  cron: '45 0 * * 1'
  class: Azurity::AzurityWorker
  queue: default
  description: "This job sends submission data to Azurity weekly"

crocs_worker:
  cron: '0 6 * * *'
  class: Crocs::CrocsWorker
  queue: default
  description: "This job sends submission data to Crocs daily"

qrvey_full_insert:
  every: '1m'
  class: Qrvey::FullInsertWorker
  queue: default
  description: "This worker uploads to an S3 bucket a CSV file Qrvey's data sync process consumes every minute"

qrvey_partial_update:
  every: '5m'
  class: Qrvey::PartialUpdateWorker
  queue: default
  description: "This worker sends metadata updates to Qrvey's Push API endpoint every 5 minutes"

nyu_langone_worker:
  cron: '0 5 * * *'
  class: NYULangone::NYULangoneWorker
  queue: default
  description: "This job sends submission data to NYU Langone daily"

summer_discovery_worker:
  cron: '0 1 * * 1'
  class: SummerDiscovery::SummerDiscoveryWorker
  queue: default
  description: "This job sends submission data to Summer Discovery weekly"
